{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import sleep\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from deep_sort_realtime.deep_sort.track import Track\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from IPython.display import clear_output\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    RetinaNet_ResNet50_FPN_V2_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn,\n",
    "    fasterrcnn_resnet50_fpn,\n",
    "    FasterRCNN_ResNet50_FPN_Weights,\n",
    "    SSDLite320_MobileNet_V3_Large_Weights,\n",
    "    ssdlite320_mobilenet_v3_large,\n",
    "    retinanet_resnet50_fpn_v2,\n",
    "    FCOS_ResNet50_FPN_Weights,\n",
    "    fcos_resnet50_fpn,\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.transforms.functional import resize\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {1}\n",
    "not 1  in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackedObject:\n",
    "    def __init__(self, id, frame_index, bounding_box):\n",
    "        self.id = id\n",
    "        self.frame_indexed = frame_index\n",
    "        self.bounding_box = bounding_box\n",
    "        self.last_frame_update = frame_index\n",
    "\n",
    "\n",
    "class ObjectTracking:\n",
    "    def __init__(self, line, frame_before_drop_track):\n",
    "        self.objects = []\n",
    "        self.frame = 0\n",
    "        self.person_count = 0\n",
    "        self.line = line\n",
    "        self.frame_before_drop_track = frame_before_drop_track\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _iou(bounding_box_a: torch.Tensor, bounding_box_b: torch.Tensor):\n",
    "    #     return box_iou(bounding_box_a.unsqueeze(0), bounding_box_b.unsqueeze(0)).numpy()\n",
    "\n",
    "    @staticmethod\n",
    "    def _distance(centroid_a, centroid_b):\n",
    "        return np.abs(centroid_a[0] - centroid_b[0]) + np.abs(\n",
    "            centroid_a[1] - centroid_b[1]\n",
    "        )\n",
    "\n",
    "    def add_object(self, bounding_box):\n",
    "        # Identify if block is tracked\n",
    "        new_object_box = bounding_box\n",
    "        closest_object = None\n",
    "        closest_iou: float = 0\n",
    "        for tracked_object in self.objects:\n",
    "            tracked_object: TrackedObject\n",
    "            print(new_object_box, tracked_object.bounding_box)\n",
    "            iou = self._iou(new_object_box, tracked_object.bounding_box)\n",
    "\n",
    "            if iou > closest_iou and iou > 0.5:\n",
    "                closest_iou = iou\n",
    "                closest_object = tracked_object\n",
    "\n",
    "        # If not match found Insert into our self.objects  Else update boundingBox\n",
    "        if closest_object is None:\n",
    "            self.objects.append(\n",
    "                TrackedObject(self.person_count, self.frame, new_object_box)\n",
    "            )\n",
    "            self.person_count += 1\n",
    "        else:\n",
    "            # Update the object with new boundingbox and frameindex\n",
    "            closest_object.bounding_box = new_object_box\n",
    "            closest_object.last_frame_update = self.frame\n",
    "\n",
    "    def track(self, bounding_boxes):\n",
    "        for bounding_box in bounding_boxes:\n",
    "            if bounding_box is None:\n",
    "                continue\n",
    "            self.add_object(bounding_box)\n",
    "        self.frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newImg = torch.permute(img, [1, 2, 0]).numpy().astype(np.uint8).copy()\n",
    "\n",
    "# h, w, _ = newImg.shape\n",
    "# xmin, ymin, xmax, ymax = prediction[\"boxes\"][0]\n",
    "# xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "# plt.imshow(cv2.rectangle(newImg, [xmin, ymin], [xmax, ymax], (255, 255, 0), 5))\n",
    "\n",
    "# tracker = ObjectTracking(line=[0, 400, 1280, 400], frame_before_drop_track=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = DeepSort(max_age=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLTWH(bounding_box):\n",
    "    width = bounding_box[2] - bounding_box[0]\n",
    "    height = bounding_box[3] - bounding_box[1]\n",
    "    return torch.Tensor([bounding_box[0], bounding_box[1], width, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam = cv2.VideoCapture('http://61.211.241.239/nphMotionJpeg?Resolution=320x240&Quality=Standard')\n",
    "cam = cv2.VideoCapture(\"rtsp://admin:@ipc.lan:554\")\n",
    "# cam = cv2.VideoCapture(\"/Users/babi/Downloads/video2.mp4\")\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    initial_time = time.time()\n",
    "    check, frame = cam.read()\n",
    "    cv2.imshow('video', frame.astype(np.uint8))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    print(f\"{round((time.time() - initial_time), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.95)\n",
    "model.eval()\n",
    "\n",
    "# # Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# cam = cv2.VideoCapture('http://61.211.241.239/nphMotionJpeg?Resolution=320x240&Quality=Standard')\n",
    "cam = cv2.VideoCapture(\"rtsp://admin:@192.168.2.245:554\")\n",
    "# cam = cv2.VideoCapture(\"/Users/babi/Downloads/video2.mp4\")\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    initial_time = time.time()\n",
    "    check, frame = cam.read()\n",
    "    img = torch.permute(torch.Tensor(frame[:, :, [2, 1, 0]]), [2, 0, 1]).to(torch.uint8)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = model(batch)[0]\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        box = draw_bounding_boxes(\n",
    "            img,\n",
    "            boxes=prediction[\"boxes\"],\n",
    "            labels=labels,\n",
    "            colors=\"red\",\n",
    "        )\n",
    "        cv2.imshow('video', torch.permute(box, [1, 2, 0]).numpy().astype(np.uint8))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        print(f\"{round((time.time() - initial_time), 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# number_person = 0\n",
    "# human_set = {}\n",
    "\n",
    "# while True:\n",
    "#     initial_time = time.time()\n",
    "#     check, frame = cam.read()\n",
    "#     img = torch.permute(torch.Tensor(frame[:, :, [2, 1, 0]]), [2, 0, 1])\n",
    "#     # batch = resize(img, [img.shape[1] // 2, img.shape[2] // 2])\n",
    "#     batch = preprocess(img).unsqueeze(0)\n",
    "#     with torch.no_grad():\n",
    "#         clear_output(wait=True)\n",
    "#         prediction = model(batch)[0]\n",
    "#         # only_human = [\n",
    "#         #     (\n",
    "#         #         convertToLTWH(prediction[\"boxes\"][i]).numpy(),\n",
    "#         #         prediction[\"labels\"][i].numpy(),\n",
    "#         #         prediction[\"scores\"][i].numpy(),\n",
    "#         #     )\n",
    "#         #     if prediction[\"labels\"][i] == 1\n",
    "#         #     else None\n",
    "#         #     for i in range(len(prediction[\"boxes\"]))\n",
    "#         # ]\n",
    "#         only_human = []\n",
    "#         for idx in range(prediction['boxes'].shape[0]):\n",
    "#             if prediction[\"labels\"][idx] == 1:\n",
    "#                 only_human.append(prediction['boxes'][idx].numpy())\n",
    "#         only_human = torch.Tensor(only_human)\n",
    "\n",
    "\n",
    "\n",
    "#         # only_human = torch.Tensor([\n",
    "#         #     prediction[\"boxes\"][i] if prediction[\"labels\"][i] == 1 else None\n",
    "#         #     for i in range(len(prediction[\"boxes\"]))\n",
    "#         # ])\n",
    "\n",
    "#         if only_human.shape[0] == 0:\n",
    "#             continue\n",
    "\n",
    "#         labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "#         box = draw_bounding_boxes(\n",
    "#             batch[0].to(torch.uint8),\n",
    "#             boxes=only_human,\n",
    "#             # labels=labels,\n",
    "#             colors=\"red\",\n",
    "#             width=4,\n",
    "#             # font_size=30,\n",
    "#         )\n",
    "\n",
    "#         img = torch.permute(batch[0], [1, 2, 0]).numpy()[..., [2, 1, 0]]\n",
    "#         img = (img * 255).astype(np.uint8)\n",
    "\n",
    "#         # tracks = tracker.update_tracks(only_human, frame=img)\n",
    "#         # print(tracks)\n",
    "\n",
    "#         # for track in tracks:\n",
    "#         #     track: Track\n",
    "#         #     if not track.is_confirmed():\n",
    "#         #         continue\n",
    "#         #     track_id = track.track_id\n",
    "#         #     ltrb = track.to_ltrb()\n",
    "\n",
    "#         #     centroid = ObjectTracking._findCentroid(ltrb)\n",
    "\n",
    "#         #     if centroid[0] > 0 and centroid[1] > 300 and track_id in human_set:\n",
    "#         #         number_person += 1\n",
    "#         #     # print(f'ID: {track_id}')\n",
    "#         #     # print(f'{ltrb}')\n",
    "\n",
    "#         # print(f'NPerson: {number_person}')\n",
    "\n",
    "#         # cv2.imshow(\"video\", torch.permute(box, [1, 2, 0]).numpy()[..., [2, 1, 0]])\n",
    "#         plt.imshow(torch.permute(box, [1, 2, 0]).numpy())\n",
    "#         plt.show()\n",
    "#         print(f\"{round( 1 / (time.time() - initial_time), 1)}\")\n",
    "#     # key = cv2.waitKey(1)\n",
    "#     # if key == 27:\n",
    "#     #     break\n",
    "# # cam.release()\n",
    "# # cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
