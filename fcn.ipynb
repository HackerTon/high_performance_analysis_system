{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from deep_sort_realtime.deep_sort.track import Track\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from IPython.display import clear_output\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# # Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# cam = cv2.VideoCapture('http://takemotopiano.aa1.netvolante.jp:8190/nphMotionJpeg?Resolution=640x480&Quality=Standard&Framerate=30')\n",
    "cam = cv2.VideoCapture('/Users/babi/Downloads/video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackedObject:\n",
    "    def __init__(self, id, frame_index, bounding_box):\n",
    "        self.id = id\n",
    "        self.frame_indexed = frame_index\n",
    "        self.bounding_box = bounding_box\n",
    "        self.last_frame_update = frame_index\n",
    "        \n",
    "\n",
    "class ObjectTracking:\n",
    "    def __init__(self, line, frame_before_drop_track):\n",
    "        self.objects = []\n",
    "        self.frame = 0\n",
    "        self.person_count = 0\n",
    "        self.line = line\n",
    "        self.frame_before_drop_track = frame_before_drop_track\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _iou(bounding_box_a: torch.Tensor, bounding_box_b: torch.Tensor):\n",
    "    #     return box_iou(bounding_box_a.unsqueeze(0), bounding_box_b.unsqueeze(0)).numpy()\n",
    "    \n",
    "    @staticmethod \n",
    "    def _distance(centroid_a, centroid_b):\n",
    "        return np.abs(centroid_a[0] - centroid_b[0])  +np.abs(centroid_a[1] - centroid_b[1])\n",
    "    \n",
    "\n",
    "    \n",
    "    def add_object(self, bounding_box):\n",
    "        # Identify if block is tracked\n",
    "        new_object_box = bounding_box\n",
    "        closest_object = None\n",
    "        closest_iou: float = 0\n",
    "        for tracked_object in self.objects:\n",
    "            tracked_object: TrackedObject\n",
    "            print(new_object_box, tracked_object.bounding_box)\n",
    "            iou = self._iou(new_object_box, tracked_object.bounding_box)\n",
    "\n",
    "            if iou > closest_iou and iou > 0.5:\n",
    "                closest_iou = iou\n",
    "                closest_object = tracked_object\n",
    "\n",
    "        # If not match found Insert into our self.objects  Else update boundingBox\n",
    "        if closest_object is None:\n",
    "            self.objects.append(TrackedObject(self.person_count, self.frame, new_object_box))\n",
    "            self.person_count += 1\n",
    "        else:\n",
    "            # Update the object with new boundingbox and frameindex\n",
    "            closest_object.bounding_box = new_object_box            \n",
    "            closest_object.last_frame_update = self.frame            \n",
    "\n",
    "    def track(self, bounding_boxes):\n",
    "        for bounding_box in bounding_boxes:\n",
    "            if bounding_box is None:\n",
    "                continue\n",
    "            self.add_object(bounding_box)\n",
    "        self.frame += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newImg = torch.permute(img, [1, 2, 0]).numpy().astype(np.uint8).copy()\n",
    "\n",
    "# h, w, _ = newImg.shape\n",
    "# xmin, ymin, xmax, ymax = prediction[\"boxes\"][0]\n",
    "# xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "# plt.imshow(cv2.rectangle(newImg, [xmin, ymin], [xmax, ymax], (255, 255, 0), 5))\n",
    "\n",
    "# tracker = ObjectTracking(line=[0, 400, 1280, 400], frame_before_drop_track=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = DeepSort(max_age=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLTWH(bounding_box):\n",
    "    width = bounding_box[2] - bounding_box[0]\n",
    "    height = bounding_box[3] - bounding_box[1]\n",
    "    return torch.Tensor([bounding_box[0], bounding_box[1], width, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/babi/Programs/real-time-analytics/fcn.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(batch[\u001b[39m0\u001b[39m], [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mnumpy()[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, [\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m img \u001b[39m=\u001b[39m (img \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m tracks \u001b[39m=\u001b[39m tracker\u001b[39m.\u001b[39;49mupdate_tracks(only_human, frame\u001b[39m=\u001b[39;49mimg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(tracks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mfor\u001b[39;00m track \u001b[39min\u001b[39;00m tracks:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/deep_sort_realtime/deepsort_tracker.py:195\u001b[0m, in \u001b[0;36mDeepSort.update_tracks\u001b[0;34m(self, raw_detections, embeds, frame, today, others, instance_masks)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(raw_detections) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolygon:\n\u001b[0;32m--> 195\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(raw_detections[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m])\u001b[39m==\u001b[39m\u001b[39m4\u001b[39m\n\u001b[1;32m    196\u001b[0m         raw_detections \u001b[39m=\u001b[39m [d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m raw_detections \u001b[39mif\u001b[39;00m d[\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m d[\u001b[39m0\u001b[39m][\u001b[39m3\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m    198\u001b[0m         \u001b[39mif\u001b[39;00m embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "number_person = 0\n",
    "human_set = {}\n",
    "while True:\n",
    "    initial_time = time.time()\n",
    "    check, frame = cam.read()\n",
    "\n",
    "    img = torch.permute(torch.Tensor(frame[:, :, [2, 1, 0]]), [2, 0, 1]).to(torch.uint8)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        clear_output(wait=True)\n",
    "        prediction = model(batch)[0]\n",
    "        only_human = [\n",
    "            (\n",
    "                convertToLTWH(prediction[\"boxes\"][i]).numpy(),\n",
    "                prediction[\"labels\"][i].numpy(),\n",
    "                prediction[\"scores\"][i].numpy(),\n",
    "            )\n",
    "            if prediction[\"labels\"][i] == 1\n",
    "            else None\n",
    "            for i in range(len(prediction[\"boxes\"]))\n",
    "        ]\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        # box = draw_bounding_boxes(\n",
    "        #     img,\n",
    "        #     boxes=prediction[\"boxes\"],\n",
    "        #     labels=labels,\n",
    "        #     colors=\"red\",\n",
    "        #     width=4,\n",
    "        #     font_size=30,\n",
    "        # )\n",
    "\n",
    "        img = torch.permute(batch[0], [1, 2, 0]).numpy()[..., [2, 1, 0]]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        tracks = tracker.update_tracks(only_human, frame=img)\n",
    "        print(tracks)\n",
    "\n",
    "        for track in tracks:\n",
    "            track: Track\n",
    "            if not track.is_confirmed():\n",
    "                continue\n",
    "            track_id = track.track_id\n",
    "            ltrb = track.to_ltrb()\n",
    "\n",
    "            centroid = ObjectTracking._findCentroid(ltrb) \n",
    "\n",
    "            if centroid[0] > 0 and centroid[1] > 300 and track_id in human_set:\n",
    "                number_person += 1\n",
    "            # print(f'ID: {track_id}')\n",
    "            # print(f'{ltrb}')\n",
    "\n",
    "        print(f'NPerson: {number_person}')\n",
    "\n",
    "    # cv2.imshow(\"video\", torch.permute(box, [1, 2, 0]).numpy()[..., [2, 1, 0]])\n",
    "    print(f\"{round( 1 / (time.time() - initial_time), 1)}\")\n",
    "    # key = cv2.waitKey(1)\n",
    "    # if key == 27:\n",
    "    #     break\n",
    "# cam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
