{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import torch\n",
    "from deep_sort_realtime.deep_sort.track import Track\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from IPython.display import clear_output\n",
    "from torchvision.models.detection import (\n",
    "    FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    fasterrcnn_mobilenet_v3_large_fpn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# # Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# cam = cv2.VideoCapture('http://takemotopiano.aa1.netvolante.jp:8190/nphMotionJpeg?Resolution=640x480&Quality=Standard&Framerate=30')\n",
    "cam = cv2.VideoCapture('/Users/babi/Downloads/video2.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackedObject:\n",
    "    def __init__(self, id, frame_index, bounding_box):\n",
    "        self.id = id\n",
    "        self.frame_indexed = frame_index\n",
    "        self.bounding_box = bounding_box\n",
    "        self.last_frame_update = frame_index\n",
    "        \n",
    "\n",
    "class ObjectTracking:\n",
    "    def __init__(self, line, frame_before_drop_track):\n",
    "        self.objects = []\n",
    "        self.frame = 0\n",
    "        self.person_count = 0\n",
    "        self.line = line\n",
    "        self.frame_before_drop_track = frame_before_drop_track\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _iou(bounding_box_a: torch.Tensor, bounding_box_b: torch.Tensor):\n",
    "    #     return box_iou(bounding_box_a.unsqueeze(0), bounding_box_b.unsqueeze(0)).numpy()\n",
    "    \n",
    "    @staticmethod \n",
    "    def _distance(centroid_a, centroid_b):\n",
    "        return np.abs(centroid_a[0] - centroid_b[0])  +np.abs(centroid_a[1] - centroid_b[1])\n",
    "    \n",
    "\n",
    "    \n",
    "    def add_object(self, bounding_box):\n",
    "        # Identify if block is tracked\n",
    "        new_object_box = bounding_box\n",
    "        closest_object = None\n",
    "        closest_iou: float = 0\n",
    "        for tracked_object in self.objects:\n",
    "            tracked_object: TrackedObject\n",
    "            print(new_object_box, tracked_object.bounding_box)\n",
    "            iou = self._iou(new_object_box, tracked_object.bounding_box)\n",
    "\n",
    "            if iou > closest_iou and iou > 0.5:\n",
    "                closest_iou = iou\n",
    "                closest_object = tracked_object\n",
    "\n",
    "        # If not match found Insert into our self.objects  Else update boundingBox\n",
    "        if closest_object is None:\n",
    "            self.objects.append(TrackedObject(self.person_count, self.frame, new_object_box))\n",
    "            self.person_count += 1\n",
    "        else:\n",
    "            # Update the object with new boundingbox and frameindex\n",
    "            closest_object.bounding_box = new_object_box            \n",
    "            closest_object.last_frame_update = self.frame            \n",
    "\n",
    "    def track(self, bounding_boxes):\n",
    "        for bounding_box in bounding_boxes:\n",
    "            if bounding_box is None:\n",
    "                continue\n",
    "            self.add_object(bounding_box)\n",
    "        self.frame += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newImg = torch.permute(img, [1, 2, 0]).numpy().astype(np.uint8).copy()\n",
    "\n",
    "# h, w, _ = newImg.shape\n",
    "# xmin, ymin, xmax, ymax = prediction[\"boxes\"][0]\n",
    "# xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "# plt.imshow(cv2.rectangle(newImg, [xmin, ymin], [xmax, ymax], (255, 255, 0), 5))\n",
    "\n",
    "# tracker = ObjectTracking(line=[0, 400, 1280, 400], frame_before_drop_track=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = DeepSort(max_age=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToLTWH(bounding_box):\n",
    "    width = bounding_box[2] - bounding_box[0]\n",
    "    height = bounding_box[3] - bounding_box[1]\n",
    "    return torch.Tensor([bounding_box[0], bounding_box[1], width, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/babi/Programs/real-time-analytics/fcn.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     prediction \u001b[39m=\u001b[39m model(batch)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     only_human \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             convertToLTWH(prediction[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m][i])\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(prediction[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/babi/Programs/real-time-analytics/fcn.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     labels \u001b[39m=\u001b[39m [weights\u001b[39m.\u001b[39mmeta[\u001b[39m\"\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m\"\u001b[39m][i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m prediction[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[1;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/models/detection/roi_heads.py:761\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    758\u001b[0m     regression_targets \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    759\u001b[0m     matched_idxs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m box_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_roi_pool(features, proposals, image_shapes)\n\u001b[1;32m    762\u001b[0m box_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_head(box_features)\n\u001b[1;32m    763\u001b[0m class_logits, box_regression \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_predictor(box_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/ops/poolers.py:314\u001b[0m, in \u001b[0;36mMultiScaleRoIAlign.forward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_levels \u001b[39m=\u001b[39m _setup_scales(\n\u001b[1;32m    311\u001b[0m         x_filtered, image_shapes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonical_scale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonical_level\n\u001b[1;32m    312\u001b[0m     )\n\u001b[0;32m--> 314\u001b[0m \u001b[39mreturn\u001b[39;00m _multiscale_roi_align(\n\u001b[1;32m    315\u001b[0m     x_filtered,\n\u001b[1;32m    316\u001b[0m     boxes,\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size,\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling_ratio,\n\u001b[1;32m    319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscales,\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmap_levels,\n\u001b[1;32m    321\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/ops/poolers.py:204\u001b[0m, in \u001b[0;36m_multiscale_roi_align\u001b[0;34m(x_filtered, boxes, output_size, sampling_ratio, scales, mapper)\u001b[0m\n\u001b[1;32m    201\u001b[0m idx_in_level \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(levels \u001b[39m==\u001b[39m level)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    202\u001b[0m rois_per_level \u001b[39m=\u001b[39m rois[idx_in_level]\n\u001b[0;32m--> 204\u001b[0m result_idx_in_level \u001b[39m=\u001b[39m roi_align(\n\u001b[1;32m    205\u001b[0m     per_level_feature,\n\u001b[1;32m    206\u001b[0m     rois_per_level,\n\u001b[1;32m    207\u001b[0m     output_size\u001b[39m=\u001b[39;49moutput_size,\n\u001b[1;32m    208\u001b[0m     spatial_scale\u001b[39m=\u001b[39;49mscale,\n\u001b[1;32m    209\u001b[0m     sampling_ratio\u001b[39m=\u001b[39;49msampling_ratio,\n\u001b[1;32m    210\u001b[0m )\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m torchvision\u001b[39m.\u001b[39m_is_tracing():\n\u001b[1;32m    213\u001b[0m     tracing_results\u001b[39m.\u001b[39mappend(result_idx_in_level\u001b[39m.\u001b[39mto(dtype))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torchvision/ops/roi_align.py:63\u001b[0m, in \u001b[0;36mroi_align\u001b[0;34m(input, boxes, output_size, spatial_scale, sampling_ratio, aligned)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(rois, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     62\u001b[0m     rois \u001b[39m=\u001b[39m convert_boxes_to_roi_format(rois)\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mtorchvision\u001b[39m.\u001b[39;49mroi_align(\n\u001b[1;32m     64\u001b[0m     \u001b[39minput\u001b[39;49m, rois, spatial_scale, output_size[\u001b[39m0\u001b[39;49m], output_size[\u001b[39m1\u001b[39;49m], sampling_ratio, aligned\n\u001b[1;32m     65\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_person = 0\n",
    "human_set = {}\n",
    "while True:\n",
    "    initial_time = time.time()\n",
    "    check, frame = cam.read()\n",
    "\n",
    "    img = torch.permute(torch.Tensor(frame[:, :, [2, 1, 0]]), [2, 0, 1]).to(torch.uint8)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        clear_output(wait=True)\n",
    "        prediction = model(batch)[0]\n",
    "        only_human = [\n",
    "            (\n",
    "                convertToLTWH(prediction[\"boxes\"][i]).numpy(),\n",
    "                prediction[\"labels\"][i].numpy(),\n",
    "                prediction[\"scores\"][i].numpy(),\n",
    "            )\n",
    "            if prediction[\"labels\"][i] == 1\n",
    "            else None\n",
    "            for i in range(len(prediction[\"boxes\"]))\n",
    "        ]\n",
    "        labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "        box = draw_bounding_boxes(\n",
    "            img,\n",
    "            boxes=prediction[\"boxes\"],\n",
    "            labels=labels,\n",
    "            colors=\"red\",\n",
    "            width=4,\n",
    "            font_size=30,\n",
    "        )\n",
    "\n",
    "        img = torch.permute(batch[0], [1, 2, 0]).numpy()[..., [2, 1, 0]]\n",
    "        img = (img * 255).astype(np.uint8)\n",
    "\n",
    "        # tracks = tracker.update_tracks(only_human, frame=img)\n",
    "        # print(tracks)\n",
    "\n",
    "        # for track in tracks:\n",
    "        #     track: Track\n",
    "        #     if not track.is_confirmed():\n",
    "        #         continue\n",
    "        #     track_id = track.track_id\n",
    "        #     ltrb = track.to_ltrb()\n",
    "\n",
    "        #     centroid = ObjectTracking._findCentroid(ltrb) \n",
    "\n",
    "        #     if centroid[0] > 0 and centroid[1] > 300 and track_id in human_set:\n",
    "        #         number_person += 1\n",
    "        #     # print(f'ID: {track_id}')\n",
    "        #     # print(f'{ltrb}')\n",
    "\n",
    "        # print(f'NPerson: {number_person}')\n",
    "\n",
    "        # cv2.imshow(\"video\", torch.permute(box, [1, 2, 0]).numpy()[..., [2, 1, 0]])\n",
    "        plt.imshow(torch.permute(box, [1, 2, 0]).numpy()[..., [2, 1, 0]])\n",
    "        plt.show()\n",
    "        # sleep(1)s\n",
    "        print(f\"{round( 1 / (time.time() - initial_time), 1)}\")\n",
    "    # key = cv2.waitKey(1)\n",
    "    # if key == 27:\n",
    "    #     break\n",
    "# cam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
